{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP1t/kgO9mIr8G6pCa2zLwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvidalf/repo-colab/blob/main/tmdb_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y dgl dgl-cu101 dgl-cu102 dgl-cu110 dgl-cu111 dgl-cu113 dgl-cu116 dgl-cu117 dgl-cu121 torch torchdata torchvision torchaudio torchtune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WX-3naEEjmi5",
        "outputId": "ed980f4b-9bd4-47ad-e1c1-774f5bde275b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: dgl 2.1.0\n",
            "Uninstalling dgl-2.1.0:\n",
            "  Successfully uninstalled dgl-2.1.0\n",
            "\u001b[33mWARNING: Skipping dgl-cu101 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu102 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu110 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu111 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu113 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu116 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu117 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dgl-cu121 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Successfully uninstalled torch-2.5.1\n",
            "Found existing installation: torchdata 0.9.0\n",
            "Uninstalling torchdata-0.9.0:\n",
            "  Successfully uninstalled torchdata-0.9.0\n",
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torchtune 0.6.1\n",
            "Uninstalling torchtune-0.6.1:\n",
            "  Successfully uninstalled torchtune-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==2.4.0 torchdata==0.8.0\n",
        "# !pip install dgl==2.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fm1l1mrPo6uj",
        "outputId": "1025ad0d-7cd4-42fc-a507-1c79601a37ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchdata==0.8.0\n",
            "  Downloading torchdata-0.8.0-cp312-cp312-manylinux1_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.8.0) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata==0.8.0) (2.32.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.8.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.8.0) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.8.0) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.8.0-cp312-cp312-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchdata\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.21 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchdata-0.8.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchdata",
                  "torchgen"
                ]
              },
              "id": "e166cae51fd74663b04d8caa2bd9e233"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl==2.1.0\n",
            "  Using cached dgl-2.1.0-cp312-cp312-manylinux1_x86_64.whl.metadata (553 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (1.16.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from dgl==2.1.0) (0.8.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl==2.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl==2.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl==2.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl==2.1.0) (2025.10.5)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchdata>=0.5.0->dgl==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata>=0.5.0->dgl==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl==2.1.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl==2.1.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl==2.1.0) (1.3.0)\n",
            "Using cached dgl-2.1.0-cp312-cp312-manylinux1_x86_64.whl (8.6 MB)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, dgl\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torchdata:\", __import__(\"torchdata\").__version__)\n",
        "print(\"DGL:\", dgl.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Backend:\", dgl.backend.backend_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "AL7k5hXLkqCe",
        "outputId": "6bab4ec5-c1d5-4a43-8c50-30fb278a98b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find DGL C++ graphbolt library at /usr/local/lib/python3.12/dist-packages/dgl/graphbolt/libgraphbolt_pytorch_2.4.0.so",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3260046190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torchdata:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchdata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DGL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_backend\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/dataloading/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preferred_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pytorch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspot_target\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/dataloading/dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbatch_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPUCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyFeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheterograph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDGLGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexit_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistGraphServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_partition_book\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphPartitionBook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartitionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/distributed/dist_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphbolt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheterograph_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mempty_shared_mem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mETYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/graphbolt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mload_graphbolt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dgl/graphbolt/__init__.py\u001b[0m in \u001b[0;36mload_graphbolt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graphbolt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34mf\"Cannot find DGL C++ graphbolt library at {path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find DGL C++ graphbolt library at /usr/local/lib/python3.12/dist-packages/dgl/graphbolt/libgraphbolt_pytorch_2.4.0.so"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "zfb2hGPloYgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JicjZC3nCPL",
        "outputId": "77385ed0-b966-44fc-8f2d-57ad04c3679e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  9 21:53:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   32C    P0             54W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "lOIzjCfW0Two"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c tmdb-box-office-prediction"
      ],
      "metadata": {
        "id": "qGiv8kDPzV33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8340837d"
      },
      "source": [
        "# !unzip /content/tmdb-box-office-prediction.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "i41botxw3umA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "dDf3wtFSErnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_columms = ['id', 'imdb_id', 'tagline']\n",
        "train_df.head(5).loc[:, ~train_df.columns.isin(excluded_columms)]"
      ],
      "metadata": {
        "id": "rX7OIQITBp8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalización de dataset\n",
        "\n",
        "Primero, dividimos el dataset en varias tablas, según las entidades identificadas"
      ],
      "metadata": {
        "id": "ad_P2oWCIZ5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Add a split indicator\n",
        "train_df[\"split\"] = \"train\"\n",
        "test_df[\"split\"] = \"test\"\n",
        "\n",
        "# Ensure both have the same schema\n",
        "for col in set(train_df.columns) - set(test_df.columns):\n",
        "    test_df[col] = None\n",
        "\n",
        "full_df = pd.concat([train_df, test_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "G-2Q_P3baq9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# -----------------------------\n",
        "# GLOBAL error log\n",
        "# -----------------------------\n",
        "errors = []\n",
        "\n",
        "# -----------------------------\n",
        "# Safe parsing\n",
        "# -----------------------------\n",
        "def safe_parse(cell, column_name, movie_id):\n",
        "    if pd.isna(cell) or cell == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except Exception as e:\n",
        "        errors.append({\n",
        "            \"movie_id\": movie_id,\n",
        "            \"column\": column_name,\n",
        "            \"error\": str(e),\n",
        "            \"value\": str(cell)[:300]\n",
        "        })\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# Generic normalization\n",
        "# -----------------------------\n",
        "def normalize_column(df, column_name, entity_id_field):\n",
        "    \"\"\"Normalize a column containing list/dict JSON-like data.\"\"\"\n",
        "    entity_rows, join_rows = [], []\n",
        "\n",
        "    for movie_id, cell in zip(df.index, df[column_name]):\n",
        "        parsed = safe_parse(cell, column_name, movie_id)\n",
        "        if not parsed:\n",
        "            continue\n",
        "        if isinstance(parsed, dict):\n",
        "            parsed = [parsed]\n",
        "\n",
        "        for entry in parsed:\n",
        "            entity_id = entry.get(entity_id_field)\n",
        "            if entity_id is None:\n",
        "                errors.append({\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"column\": column_name,\n",
        "                    \"error\": f\"Missing {entity_id_field}\",\n",
        "                    \"value\": str(entry)\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            entity_rows.append(entry)\n",
        "            join_rows.append({\"movie_id\": movie_id, entity_id_field: entity_id})\n",
        "\n",
        "    entity_df = pd.DataFrame(entity_rows).drop_duplicates(subset=[entity_id_field])\n",
        "    join_df = pd.DataFrame(join_rows).drop_duplicates()\n",
        "    return entity_df.reset_index(drop=True), join_df.reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Specialized normalization for cast/crew\n",
        "# -----------------------------\n",
        "def normalize_people(df, column_name, role_type):\n",
        "    \"\"\"Extract people (cast or crew) as separate entity + join tables.\"\"\"\n",
        "    person_rows, join_rows = [], []\n",
        "\n",
        "    for movie_id, cell in zip(df.index, df[column_name]):\n",
        "        parsed = safe_parse(cell, column_name, movie_id)\n",
        "        if not parsed:\n",
        "            continue\n",
        "\n",
        "        for entry in parsed:\n",
        "            pid = entry.get(\"id\")\n",
        "            if not pid:\n",
        "                errors.append({\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"column\": column_name,\n",
        "                    \"error\": \"Missing person id\",\n",
        "                    \"value\": str(entry)\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            person_rows.append({\n",
        "                \"person_id\": pid,\n",
        "                \"name\": entry.get(\"name\"),\n",
        "                \"gender\": entry.get(\"gender\"),\n",
        "                \"profile_path\": entry.get(\"profile_path\")\n",
        "            })\n",
        "\n",
        "            if role_type == \"cast\":\n",
        "                join_rows.append({\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"person_id\": pid,\n",
        "                    \"character\": entry.get(\"character\"),\n",
        "                    \"order\": entry.get(\"order\"),\n",
        "                    \"credit_id\": entry.get(\"credit_id\"),\n",
        "                    \"cast_id\": entry.get(\"cast_id\")\n",
        "                })\n",
        "            else:\n",
        "                join_rows.append({\n",
        "                    \"movie_id\": movie_id,\n",
        "                    \"person_id\": pid,\n",
        "                    \"department\": entry.get(\"department\"),\n",
        "                    \"job\": entry.get(\"job\"),\n",
        "                    \"credit_id\": entry.get(\"credit_id\")\n",
        "                })\n",
        "\n",
        "    people_df = pd.DataFrame(person_rows).drop_duplicates(subset=[\"person_id\"])\n",
        "    join_df = pd.DataFrame(join_rows).drop_duplicates()\n",
        "    return people_df.reset_index(drop=True), join_df.reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "9TVfsEuOIZR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "def normalize_dataframe(df):\n",
        "    \"\"\"Normalize a movie metadata dataframe into relational tables.\"\"\"\n",
        "    df = df.copy()\n",
        "    df.rename(columns={\"Keywords\": \"keywords\"}, inplace=True)\n",
        "\n",
        "    # --- Define normalization schema ---\n",
        "    Column = namedtuple(\n",
        "        \"Column\",\n",
        "        [\"name\", \"entity_id_field\", \"entity_df_name\", \"join_df_name\", \"is_people\"]\n",
        "    )\n",
        "\n",
        "    columns_to_normalize = [\n",
        "        Column(\"genres\", \"id\", \"genres\", \"movie_genres\", False),\n",
        "        Column(\"production_companies\", \"id\", \"companies\", \"movie_companies\", False),\n",
        "        Column(\"keywords\", \"id\", \"keywords\", \"movie_keywords\", False),\n",
        "        Column(\"belongs_to_collection\", \"id\", \"collections\", \"movie_collections\", False),\n",
        "        Column(\"cast\", \"id\", \"people\", \"movie_cast\", True),\n",
        "        Column(\"crew\", \"id\", \"people\", \"movie_crew\", True),\n",
        "        Column(\"production_countries\", \"iso_3166_1\", \"countries\", \"movie_countries\", False),\n",
        "        Column(\"spoken_languages\", \"iso_639_1\", \"languages\", \"movie_languages\", False),\n",
        "    ]\n",
        "\n",
        "    final_dataframes = {}\n",
        "    all_people = []\n",
        "\n",
        "    # --- Normalize all relational / list-like columns ---\n",
        "    for col in columns_to_normalize:\n",
        "        print(f\"Normalizing {col.name}...\")\n",
        "        if col.is_people:\n",
        "            people_df, join_df = normalize_people(df, col.name, col.name)\n",
        "            all_people.append(people_df)\n",
        "            final_dataframes[col.join_df_name] = join_df\n",
        "        else:\n",
        "            entity_df, join_df = normalize_column(df, col.name, col.entity_id_field)\n",
        "            entity_col = f\"{col.entity_df_name[:-1]}_id\"\n",
        "            entity_df.rename(columns={col.entity_id_field: entity_col}, inplace=True)\n",
        "            join_df.rename(columns={col.entity_id_field: entity_col}, inplace=True)\n",
        "            final_dataframes[col.entity_df_name] = entity_df\n",
        "            final_dataframes[col.join_df_name] = join_df\n",
        "\n",
        "        # Drop the column from the base df after it’s processed\n",
        "        if col.name in df.columns:\n",
        "            df.drop(columns=[col.name], inplace=True)\n",
        "\n",
        "    # --- Merge all people into a single unique table ---\n",
        "    if all_people:\n",
        "        people_df = pd.concat(all_people).drop_duplicates(subset=[\"person_id\"])\n",
        "        final_dataframes[\"people\"] = people_df\n",
        "\n",
        "    # --- Now the remaining df is our movies table ---\n",
        "    movies_df = df.copy()\n",
        "\n",
        "    # Rename ID column\n",
        "    if \"id\" in movies_df.columns:\n",
        "        movies_df.rename(columns={\"id\": \"movie_id\"}, inplace=True)\n",
        "\n",
        "    # -- Process table by dropping and renaming columns ---\n",
        "    cols_to_ignore = [\"homepage\", \"poster_path\"]\n",
        "    movies_df.drop(columns=[c for c in cols_to_ignore if c in movies_df.columns], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    movies_df.reset_index(drop=True, inplace=True)\n",
        "    if \"movie_id\" in movies_df.columns:\n",
        "        movies_df[\"movie_id\"] = pd.to_numeric(movies_df[\"movie_id\"], errors=\"coerce\")\n",
        "\n",
        "    final_dataframes[\"movies\"] = movies_df\n",
        "\n",
        "    # --- Wrap up ---\n",
        "    print(f\"✅ Normalization complete. {len(errors)} errors logged.\")\n",
        "    if errors:\n",
        "        log_name = \"normalization_errors.csv\"\n",
        "        pd.DataFrame(errors).to_csv(log_name, index=False)\n",
        "        print(f\"⚠️ Errors saved to {log_name}\")\n",
        "\n",
        "    return final_dataframes"
      ],
      "metadata": {
        "id": "RRTiaI_GItde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dfs = normalize_dataframe(full_df)"
      ],
      "metadata": {
        "id": "LbehHZf7AJzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_dfs.keys())"
      ],
      "metadata": {
        "id": "3wuqTbFoBPkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_final_dataframes(final_dataframes, n=10):\n",
        "    \"\"\"Prints the first n rows of each dataframe in final_dataframes.\"\"\"\n",
        "    for name, df in final_dataframes.items():\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"📄 DataFrame: {name}  |  Shape: {df.shape}\")\n",
        "        print(\"-\" * 80)\n",
        "        # Show first n rows (truncate long values for clarity)\n",
        "        with pd.option_context('display.max_columns', None,\n",
        "                               'display.width', 1000,\n",
        "                               'display.max_colwidth', 80):\n",
        "            print(df.head(n))\n",
        "        print(\"\\n\")\n",
        "\n",
        "preview_final_dataframes(full_dfs)\n"
      ],
      "metadata": {
        "id": "ZdAutKacKeOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construcción de grafo heterogéneo\n",
        "\n",
        "Usando dgl, construimos el grafo declarando entidades y relaciones"
      ],
      "metadata": {
        "id": "mEmmR42MbTEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "\n",
        "def build_dgl_graph(dfs):\n",
        "    # --- Create contiguous ID maps ---\n",
        "    id_maps = {}\n",
        "    for name, df in dfs.items():\n",
        "        if name.startswith(\"movie_\"):\n",
        "            continue\n",
        "        if name == \"movies\":\n",
        "            id_maps[\"movie\"] = {mid: i for i, mid in enumerate(df[\"movie_id\"].values)}\n",
        "        elif name == \"people\":\n",
        "            id_maps[\"person\"] = {pid: i for i, pid in enumerate(df[\"person_id\"].values)}\n",
        "        elif name == \"genres\":\n",
        "            id_maps[\"genre\"] = {gid: i for i, gid in enumerate(df[\"genre_id\"].values)}\n",
        "        elif name == \"companies\":\n",
        "            id_maps[\"company\"] = {cid: i for i, cid in enumerate(df[\"companie_id\"].values)}\n",
        "        elif name == \"collections\":\n",
        "            id_maps[\"collection\"] = {cid: i for i, cid in enumerate(df[\"collection_id\"].values)}\n",
        "        elif name == \"keywords\":\n",
        "            id_maps[\"keyword\"] = {kid: i for i, kid in enumerate(df[\"keyword_id\"].values)}\n",
        "        elif name == \"countries\":\n",
        "            id_maps[\"country\"] = {cid: i for i, cid in enumerate(df[\"countrie_id\"].values)}\n",
        "        elif name == \"languages\":\n",
        "            id_maps[\"language\"] = {lid: i for i, lid in enumerate(df[\"language_id\"].values)}\n",
        "\n",
        "    # --- Build edge data ---\n",
        "    relations = {\n",
        "        ('movie', 'has_genre', 'genre'): ('movie_genres', 'movie_id', 'genre_id'),\n",
        "        ('movie', 'produced_by', 'company'): ('movie_companies', 'movie_id', 'companie_id'),\n",
        "        ('movie', 'has_keyword', 'keyword'): ('movie_keywords', 'movie_id', 'keyword_id'),\n",
        "        ('movie', 'has_cast', 'person'): ('movie_cast', 'movie_id', 'person_id'),\n",
        "        ('movie', 'has_crew', 'person'): ('movie_crew', 'movie_id', 'person_id'),\n",
        "        ('movie', 'belongs_to_collection', 'collection'): ('movie_collections', 'movie_id', 'collection_id'),\n",
        "        ('movie', 'made_in', 'country'): ('movie_countries', 'movie_id', 'countrie_id'),\n",
        "        ('movie', 'in_language', 'language'): ('movie_languages', 'movie_id', 'language_id'),\n",
        "    }\n",
        "\n",
        "    graph_data = {}\n",
        "\n",
        "    for (src_type, etype, dst_type), (join_name, src_field, dst_field) in relations.items():\n",
        "        if join_name not in dfs:\n",
        "            continue\n",
        "        join_df = dfs[join_name]\n",
        "        src_ids = join_df[src_field].map(id_maps[src_type]).dropna().astype(int)\n",
        "        dst_ids = join_df[dst_field].map(id_maps[dst_type]).dropna().astype(int)\n",
        "        graph_data[(src_type, etype, dst_type)] = (src_ids.values, dst_ids.values)\n",
        "\n",
        "    # --- Create heterogeneous graph ---\n",
        "    g = dgl.heterograph(graph_data)\n",
        "    return g, id_maps\n"
      ],
      "metadata": {
        "id": "5h-65vSKbRwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g, id_maps = build_dgl_graph(full_dfs)\n",
        "print(g)"
      ],
      "metadata": {
        "id": "hyipHwCvcA0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de features\n",
        "\n",
        "Mediante 3 handlers: Numeric, categorical y text, codificamos features para todas las entidades, dependiendo del tipo de columnas que manejen"
      ],
      "metadata": {
        "id": "-BB5vcStd2C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Common encoders for numeric, categorical, text\n",
        "# ------------------------------------------------------------\n",
        "def scale_numeric(df, cols):\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(df[cols].fillna(0.0))\n",
        "\n",
        "def encode_categorical(df, cols):\n",
        "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "    return ohe.fit_transform(df[cols].fillna(\"UNK\"))\n",
        "\n",
        "def encode_text(df, col):\n",
        "    texts = df[col].fillna(\"\").astype(str).values\n",
        "    features = np.zeros((len(texts), 2), dtype=np.float32)\n",
        "    for i, t in enumerate(texts):\n",
        "        features[i, 0] = len(t)\n",
        "        features[i, 1] = sum(c.isupper() for c in t) / max(len(t), 1)\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(features)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MOVIE NODE FEATURES\n",
        "# ------------------------------------------------------------\n",
        "def add_movie_features(g, dfs, id_maps):\n",
        "    movies = dfs[\"movies\"].copy().set_index(\"movie_id\")\n",
        "\n",
        "    numeric_cols = [\"budget\", \"popularity\", \"runtime\", \"revenue\"]\n",
        "    cat_cols     = [\"original_language\", \"status\"]\n",
        "\n",
        "    num_feats = scale_numeric(movies, numeric_cols)\n",
        "    cat_feats = encode_categorical(movies, cat_cols)\n",
        "    X = np.concatenate([num_feats, cat_feats], axis=1)\n",
        "\n",
        "    g.nodes[\"movie\"].data[\"feat\"] = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "    # Masks + label\n",
        "    train_ids = movies.query(\"split == 'train'\").index.map(id_maps[\"movie\"]).dropna().astype(int).values\n",
        "    test_ids  = movies.query(\"split == 'test'\").index.map(id_maps[\"movie\"]).dropna().astype(int).values\n",
        "    train_mask = torch.zeros(g.num_nodes(\"movie\"), dtype=torch.bool)\n",
        "    test_mask  = torch.zeros(g.num_nodes(\"movie\"), dtype=torch.bool)\n",
        "    train_mask[train_ids] = True\n",
        "    test_mask[test_ids]   = True\n",
        "\n",
        "    g.nodes[\"movie\"].data[\"train_mask\"] = train_mask\n",
        "    g.nodes[\"movie\"].data[\"test_mask\"]  = test_mask\n",
        "    labels = movies[\"revenue\"].fillna(0.0).values\n",
        "    g.nodes[\"movie\"].data[\"label\"] = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    print(f\"✅ movie features: {X.shape[1]} dims, {len(train_ids)} train / {len(test_ids)} test\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# GENERIC TEXTUAL NODE FEATURES\n",
        "# ------------------------------------------------------------\n",
        "def add_textual_features(g, dfs, id_maps, entity_name, df_key, text_col):\n",
        "    df = dfs[df_key]\n",
        "    feats = encode_text(df, text_col)\n",
        "    g.nodes[entity_name].data[\"feat\"] = torch.tensor(feats, dtype=torch.float32)\n",
        "    print(f\"✅ {entity_name} features: {feats.shape[1]} dims ({len(df)} nodes)\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PEOPLE NODE FEATURES\n",
        "# ------------------------------------------------------------\n",
        "def add_people_features(g, dfs, id_maps):\n",
        "    df = dfs[\"people\"].copy()\n",
        "    df[\"gender\"] = pd.to_numeric(df[\"gender\"], errors=\"coerce\").fillna(0)\n",
        "    gender_feat = df[\"gender\"].to_numpy().reshape(-1, 1)\n",
        "    name_feats = encode_text(df, \"name\")\n",
        "    X = np.concatenate([gender_feat, name_feats], axis=1)\n",
        "    g.nodes[\"person\"].data[\"feat\"] = torch.tensor(X, dtype=torch.float32)\n",
        "    print(f\"✅ person features: {X.shape[1]} dims ({len(df)} nodes)\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EDGE FEATURES (using same encoding functions)\n",
        "# ------------------------------------------------------------\n",
        "def add_edge_features(g, dfs, id_maps, edge_type, df_key, numeric_cols=None, cat_cols=None, text_cols=None):\n",
        "    df = dfs[df_key].copy()\n",
        "    src_t, etype, dst_t = edge_type\n",
        "    src_map = id_maps[src_t]\n",
        "    dst_map = id_maps[dst_t]\n",
        "\n",
        "    df = df[df.iloc[:,0].map(src_map).notna() & df.iloc[:,1].map(dst_map).notna()]\n",
        "    feature_blocks = []\n",
        "\n",
        "    if numeric_cols:\n",
        "        feature_blocks.append(scale_numeric(df, numeric_cols))\n",
        "    if cat_cols:\n",
        "        feature_blocks.append(encode_categorical(df, cat_cols))\n",
        "    if text_cols:\n",
        "        blocks = [encode_text(df, c) for c in text_cols]\n",
        "        feature_blocks.extend(blocks)\n",
        "\n",
        "    if not feature_blocks:\n",
        "        feature_blocks = [np.zeros((len(df), 1))]\n",
        "\n",
        "    feats = np.concatenate(feature_blocks, axis=1)\n",
        "    g.edges[edge_type].data[\"feat\"] = torch.tensor(feats, dtype=torch.float32)\n",
        "    print(f\"✅ edge {etype}: {feats.shape[1]} dims ({len(df)} edges)\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Attach features for all entities\n",
        "# ------------------------------------------------------------\n",
        "add_movie_features(g, normalized, id_maps)\n",
        "\n",
        "add_textual_features(g, normalized, id_maps, \"genre\",      \"genres\",      \"name\")\n",
        "add_textual_features(g, normalized, id_maps, \"company\",    \"companies\",   \"name\")\n",
        "add_textual_features(g, normalized, id_maps, \"keyword\",    \"keywords\",    \"name\")\n",
        "add_textual_features(g, normalized, id_maps, \"collection\", \"collections\", \"name\")\n",
        "add_textual_features(g, normalized, id_maps, \"country\",    \"countries\",   \"name\")\n",
        "add_textual_features(g, normalized, id_maps, \"language\",   \"languages\",   \"name\")\n",
        "add_people_features(g, normalized, id_maps)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Attach edge features (same handlers)\n",
        "# ------------------------------------------------------------\n",
        "add_edge_features(g, normalized, id_maps, ('movie','has_cast','person'),\n",
        "                  'movie_cast',\n",
        "                  numeric_cols=['order'],\n",
        "                  text_cols=['character'])\n",
        "\n",
        "add_edge_features(g, normalized, id_maps, ('movie','has_crew','person'),\n",
        "                  'movie_crew',\n",
        "                  cat_cols=['department', 'job'])\n",
        "\n",
        "# Dummy zeros for other relations (consistency)\n",
        "for etype in g.etypes:\n",
        "    if \"feat\" not in g.edges[etype].data:\n",
        "        g.edges[etype].data[\"feat\"] = torch.zeros((g.num_edges(etype), 1))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Save and summarize\n",
        "# ------------------------------------------------------------\n",
        "dgl.save_graphs(\"movie_graph_full.bin\", [g])\n",
        "print(\"\\n💾 Saved → movie_graph_full.bin\")\n",
        "\n",
        "for ntype in g.ntypes:\n",
        "    print(f\"Node[{ntype:10}] feat_dim={g.nodes[ntype].data['feat'].shape[1]}\")\n",
        "for etype in g.etypes:\n",
        "    print(f\"Edge[{etype:25}] feat_dim={g.edges[etype].data['feat'].shape[1]}\")\n"
      ],
      "metadata": {
        "id": "G42JyIQVd2ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_graph_to_csv(final_dataframes, output_dir=\"graph_data\"):\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Export node tables\n",
        "    node_types = [\"movies\", \"people\", \"genres\", \"companies\", \"collections\", \"keywords\"]\n",
        "    for node in node_types:\n",
        "        if node in final_dataframes:\n",
        "            final_dataframes[node].to_csv(f\"{output_dir}/{node}.csv\", index=False)\n",
        "\n",
        "    # Export edge tables\n",
        "    for name, df in final_dataframes.items():\n",
        "        if name.startswith(\"movie_\"):\n",
        "            df.to_csv(f\"{output_dir}/{name}.csv\", index=False)\n",
        "\n",
        "    print(f\"✅ Graph exported to '{output_dir}/' as CSVs.\")"
      ],
      "metadata": {
        "id": "_vw0QgjKG1Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(\"/content/graph_data\")"
      ],
      "metadata": {
        "id": "_SJ8nn3AQ_fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_graph_to_csv(full_dfs)"
      ],
      "metadata": {
        "id": "nqZlNgzDG2NF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}